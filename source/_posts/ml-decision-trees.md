---
title: (机器学习) 决策树算法与 Python 实现
date: 2016-12-04 11:11:41
tags: [Machine Learning, Math]
---

## ID3 算法

### 信息增益 Information Gain

假设训练数据集为 $D$，$| D |$ 表示其样本容量；结果有 $K$ 个分类 $C_1, C_2, ..., C_K$，$ | C_i | $ 为类 $C_i$ 的大小，则 $\sum  | C_i |  =  | D | $。

假特征 $A$ 有 $n$ 个不同的取值，按照 $A$ 将 $D$ 划分为 $n$ 个子集 $D_1, D_2, ..., D_n$，$ | D_i | $ 为 $D_i$ 的容量，则 $\sum  | D_i |  =  | D | $。记 $D_i$ 中属于类 $C_j$ 的样本集合为 $D_{ij}$，则当前数据集 $D$ 的经验熵为

$$H(D) = - \sum_{k=1}^K \frac{ | C_k | }{ | D | } log \frac{ | C_k | }{ | D | }$$

特征 $A$ 对数据集 $D$ 的经验条件熵：

$$H(D | A) = \sum_{i=1}^n \frac{ | D_i | }{ | D | } H(D_i) = - \sum_{i=1}^n  \frac{ | D_i | }{ | D | } \sum_{k=1}^K \frac{ | D_{ik} | }{ | D_i | } log \frac{ | D_{ik} | }{ | D_i | }$$

条件经验熵即**各个子集的经验熵，按照子集的样本数量占比的加权求和**。

信息增益的计算方法：

$$g(D, A) = H(D) - H(D | A)$$

> 给定训练数据集 $D$ 和特征 $A$ ，经验熵 $H(D)$ 表示对数据集 $D$ 进行分类的不确定性。而经验条件熵 $H(D | A)$ 表示在特征 $A$ 给定的条件下对数据集 $D$ 进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征 $A$ 而使得数据集 $D$ 的分类的不确定性减少的程度。

<!--more-->

### 离散特征算法

ID3适用于离散特征，其特点是：**当使用特征 $A$ 进行划分时， $A$ 的所有取值都会有独立的分支，即多分支树**。算法步骤如下：

1. 若 $D$ 中所有实例都属于同一类 $C_k$，则 $T$ 为单节点树，并将类 $C_k$ 作为该节点的类标记，返回 $T$。
2. 若 $A = \Phi$，则 $T$ 为单节点树，并将 $D$ 中实例最大的类 $C_k$ 作为该节点的类标记，返回 $T$。
3. 否则，按照信息增益的算法，计算每个特征对 $D$ 的信息增益，取信息增益最大的特征 $A_g$。
4. 判断最大信息增益是否满足阈值 $\epsilon$：如果 $A_g$ 的信息增益小于阈值 $\epsilon$，则置 $T$ 为单节点树，并将 $D$ 中实例最大的类 $C_k$ 作为该节点的类标记，返回 $T$ 。
5. 否则：对 $A_g$ 的每一可能值 $a_i$，依 $A_g = a_i$ 将 $D$ 分成若干非空子集 $D_i$，将 $D_i$ 中实例最大的类作为标记；构建子节点，由节点和子节点构成树 $T$，返回 $T$。
6. 对第 $i$ 个子节点，以 $D_i$ 为训练集，以 $A - \{A_g\}$ 为特征集，递归调用*步骤1 ~ 步骤5*，得到子树 $T_i$，返回 $T_i$。

## C4.5 算法

### 信息增益比 Gain Ratio

ID3中的信息增益准则对可取值数目较多的属性有所偏好。C4.5对ID3的主要改进，是采用信息增益比替代信息增益作为判断条件。信息增益比的定义

$$g_R(D, A) = \frac{g(D, A)}{IV(D)}$$
$$IV(A) = - \sum_{i=1}^n  \frac{ | D_i | }{ | D | } log \frac{ | D_i | }{ | D | }$$

其中 $IV(A)$ 称为属性 $A$ 的固有值，属性 $A$ 的可取值越多，$IV(A)$ 的值越大。

### 离散特征算法

算法步骤同ID3，不同的是：使用信息增益比选择最佳划分特征。

### 连续特征算法

C4.5算法支持连续变量特征，与离散变量的`多分支树`不同，连续变量采用`二分支树`方案，具体步骤为：

> 1. 把需要处理的样本或样本子集按照连续变量的大小从小到大进行排序。
> 2. 假设该属性对应的不同的属性值一共有 $N$ 个，那么总共有 $N-1$ 个候选分割阈值点，每个候选的分割阈值点的值选为上述排序后的属性值中相邻连续元素的中点，根据该分割点将样本分为2组。
> 3. 用Gain Ratio选择最佳划分特征。

## CART 算法

### 基尼指数 Gini Index

相比ID3的信息增益、C4.5的信息增益比，CART分类树用的是基尼指数。对于给定的样本集合 $D$ ，基尼指数为：

$$Gini(D) = 1 - \sum_{k=1}^K (\frac{ | C_k | }{ | D | })^2$$

其中 $C_k$ 是 $D$ 中属于第 $k$ 类的样本子集， $K$ 为类的个数。

如果样本集合 $D$ 被某个特征 $A$ 是否取某个值分成两个样本集合 $D_1$ 和 $D_2$，则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为：

$$Gini(D,A) = \frac{ | D_1 | }{ | D | } Gini(D_1) + \frac{ | D_2 | }{ | D | } Gini(D_2)$$

即子集基尼系数的加权平均。

> **基尼系数与信息增益的比较**，根据 [Theoretical comparison between the Gini Index and Information Gain criteria](http://www.ius-migration.ch/files/content/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf) 的结论，在穷举的各种组合情况下，二者不一致的比例占2%，而且无法判定最优选择。

### 算法

步骤同ID3，区别是在离散变量特征划分时，按照基尼指数最大减少方案划分为2个子集；即对**连续和离散变量特征均采用二分支树。**

## 用于回归问题的决策树

当输出变量 $Y$ 为连续变量，回归树对应着输入空间的划分以及在划分单元上的输出。假如空间的划分为 $M$ 个单元 $R_1, R_2, ..., R_M$，在每个单元 $R_m$ 上又固定输出 $c_m$，通常取值为该划分内的 $Y$ 的均值，即： 

$$c_m = \frac{Y \cdot I(X \in R_m)}{ | R_m | }$$

回归树可以表示为：

$$f(X) = \sum_{m=1}^M c_m I(X \in R_m)$$

划分原则：最小化各个分支的组内误差平方和之和。以对于连续特征的二叉树划分为例：假设用第 $j$ 个特征 $X_j$ 的取值 $s$ 作为划分点，划分的两个区域为：

$$R_1(j,s) = \{X | X_j \le s\}, \ R_2(j,s) = \{X | X_j \gt s\}$$

则损失函数为：

$$min_{j,s} [\sum_{X^i \in R_1(j,s)} (y^i - c_1)^2 + \sum_{X^i \in R_2(j,s)} (y^i - c_2)^2]$$

寻找所有 $(j,s)$ 中使损失函数最小的组合；对划分后的子区域重复这一步骤。

## 剪枝

### 前剪枝

前剪枝对信息增益等划分条件设定阈值，未达到阈值时，不做进一步划分。前剪枝的缺点是：**可能错过满足阈值的后续划分。**

### 后剪枝

完全生长后，从终端节点开始向上剪枝。一般情况下，后剪枝的决策树会比前剪枝的更大。

#### ID3 与 C4.5 的剪枝算法

决策树的剪枝通过极小化损失函数实现。设树 $T$ 的叶结点个数为 $| T |$ ， $t$ 数树 $T$ 的叶结点，改叶结点有 $N_t$ 个样本点，其中 $k$ 类的样本点有 $N_{tk}$ 个， $k=1,2,...,K$ ， $H_t(T)$ 为叶结点 $t$ 上的经验熵， $\alpha \ge 0$ 为参数，则损失函数可以定义为：

$$C_{\alpha}(T) = \sum_{t=1}^{| T |} N_t H_t(T) + \alpha | T |$$

其中经验熵为：

$$H_t(T) = - \sum_k \frac{N_{tk}}{N_t} log \frac{N_{tk}}{N_t}$$

将损失函数右端的第1项记为 $C(T)$ ，则有：

$$C_{\alpha}(T) = C(T) + \alpha | T |$$

剪枝算法：

1. 计算每个结点的经验熵。
2. 递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体树分别为 $T_B$ 与 $T_A$ 且损失函数分别为 $C_{\alpha}(T_B)$ 与 $C_{\alpha}(T_A)$ ，如果 $C_{\alpha}(T_A) \le C_{\alpha}(T_B)$ 则进行剪枝，即将父结点作为新的叶结点。
3. 重复*步骤 2*，直至不能继续为止，得到损失函数最小的子树 $T_{\alpha}$ 。

#### CART 的剪枝算法

比较任意一个内部结点 $t$ 和以它作为根结点的子树 $T_t$ 的损失函数：

$$C_{\alpha}(T) = C(T) + \alpha$$

$$C_{\alpha}(T_t) = C(T_t) + \alpha | T_t |$$

当 $\alpha$ 从0开始增大，在某一 $\alpha$ 有：

$$\alpha = \frac{C(t) - C(T_t)}{| T_t | - 1}$$

上式右侧记为 $g(t)$ 表示剪枝后整体损失函数减少的程度。

在 $T_0$ 中剪去 $g(t)$ 最小的 $T_t$ ，同时将最小的 $g(t)$ 设为 $\alpha_1$ 。不断增加 $\alpha$ 的值，重复剪枝得到最优的子树序列 ${T_0, T_1, ..., T_n}$ ，对应正则化参数序列 $0 < \alpha_1 < \alpha_2 < ... < \alpha_n$ 。

在剪枝得到的子树序列 ${T_0, T_1, ..., T_n}$ 中，通过交叉验证选取最优子树 $T_{\alpha}$ 。

剪枝算法：

1. 设 $k = 0$ ， $T = T_0$ 。
2. 设 $\alpha = +\infty$ 。
3. 自下而上地对各内部结点 $t$ 计算 $C(T_t)$ ， $| T_t |$ 以及 $g(t)  = \frac{C(t) - C(T_t)}{| T_t | - 1}$ ， $\alpha = min(\alpha, g(t))$ 。这里， $T_t$ 表示以 $t$ 为根节点的子树， $C(T_t)$ 是对训练数据的预测误差， $T_t$ 是 $T_t$ 的叶结点个数。
4. 自上而下第访问内部结点 $t$ ，如果有 $g(t) = \alpha$ ，进行剪枝，并对叶结点 $t$ 以多数表决法决定其类，得到树 $T$ 。
5. 设 $k = k + 1$ ， $\alpha_k = \alpha$ ， $T_k = T$ 。
6. 如果 $T$ 不是由根结点单独构成的树，则回到*步骤 4*。
7. 采用交叉验证法在子树序列 ${T_0, T_1, ..., T_n}$ 中选取左右的子树 $T_{\alpha}$ 。

## 空值处理

以下说明C4.5算法中对空值的处理。

### 训练样本空值

决定划分特征：按照非空值计算信息增益比，当计算某个特征的信息增益时，在非空值记录的计算结果上，乘以该特征下非空值样本的比例。当划分特征和条件确定后，假设划分特征为 $A$，对于特征 $A$ 为空值的样本，发送到划分后的每一个子集。这种方案的优点是：最大化利用样本中的其他非空值特征。

### 预测样本空值

**当预测样本行走到空值特征时，将其同时发送到所有子集，并将所有子集对样本的预测结果，按照子集的样本数加权平均**。

## Python 实现

递归构建n分支树，实现代码参见 [GitHub](https://github.com/ferris-wufei/algorithm_ml/blob/master/ml_DTS.py)。
