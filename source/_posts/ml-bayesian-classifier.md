---
title: (机器学习) 朴素贝叶斯分类器
date: 2016-12-04 07:47:02
tags: [Machine Learning, Math]
---

`naive Bayes` 选取后验概率最大的分类标签，作为预测输出值。过程主要使用条件概率；手算即可，就不写代码了。

## 算法

输入：训练集 $D = \{(x_1, y_1), (x_2, y_2), …, (x_m, y_m\}$ ，其中 $x_i = (x_i^{(1)},x_i^{(2)}, …, x_i^{(n)})^T$ ，其中 $x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征，$x_i^{(j)} \in \{a_{j1},a_{j2},…,a_{jS_j}\}$，$a_{jl}$ 是第 $j$ 个特征的第 $l$ 个可能的取值。$y_i \in \{ c_1,c_2,…,c_K \}$。实例 $x$。

输出：实例 $x$ 的分类

(1) 计算先验概率和条件概率

$$P(Y=c_k) = \frac{\sum_{i=1}^m I(y_i = c_k)}{m}, \ k=1,2,…,K​$$

$$P(X^{(j)}=a_{jl} | Y=c_k) = \frac{\sum_{i=1}^m I(x_i^{(j)}=a_{jl}, y_i = c_k)}{\sum_{i=1}^m I(y_i = c_k)}$$

$$j = 1,2,…,n; \ l=1,2,…,S_j; \ k=1,2,…,K$$

<!-- more -->

(2) 对于给定的实例 $x = (x^{(1)},x^{(2)},…,x^{(n)})^T$，计算每个分类的后验概率。

首先根据条件概率的定义

$$P(Y=c_k | X = x) = \frac{P(Y=c_k, X=x)}{P(X = x)} = \frac{P(X=x | Y=c_k)P(Y=c_k)}{\sum_k P(X=x | Y=c_k)P(Y=c_k)}$$

注意到分母对于所有的 $c_k$ 相同，因此只需比较不同 $c_k$ 的分子

> $x$ 是 $n$ 维向量，朴素贝叶斯的基本假设是：输入空间的各个分量，对输出空间的所有值条件独立。

由条件独立假设，联合概率等于各随机变量概率的乘积；因此以上分子可以表示为

$$P(Y=c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} | Y=c_k), \ k=1,2,…,K$$

(3) 实例 $x$ 的分类，取以上后验概率中，分子的值最大的类

$$y = arg\max_{c_k} P(Y=c_k) \prod_{j=1}^n P(X^{(j)} = x^{(j)} | Y=c_k)$$

### 拉普拉斯平滑

计算条件概率时，可能会出现概率值为0的情况。解决方法：在随机变量的各个取值的频数上赋予一个正数 $\lambda > 0$，当取值 $\lambda = 1$ 时称为`拉普拉斯平滑`。对应地，先验概率和条件概率计算修改如下

$$P_{\lambda}(Y=c_k) = \frac{\sum_{i=1}^m I(y_i = c_k) + \lambda}{m + K \lambda}, \ k=1,2,…,K$$

$$P(X^{(j)}=a_{jl} | Y=c_k) = \frac{\sum_{i=1}^m I(x_i^{(j)}=a_{jl}, y_i = c_k) + \lambda}{\sum_{i=1}^m I(y_i = c_k) + S_j \lambda}$$