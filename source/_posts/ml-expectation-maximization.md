---
title: (机器学习) 期望极大 EM 算法与 Python 实现
date: 2016-12-04 16:00:28
tags: [Machine Learning, Math]
---

## 算法

### 个人理解

EM 可以作为 k-means 的一种替代方案。EM 将样本看做是由 k 个参数不同的多元分布生成的，我们的任务就是通过迭代算法，估计每个多元分布的参数。迭代过程：与 k-means 类似，对样本所属的分类随机初始化之后，求出参数的初始化值，通过 E 步估算出在每个样本下，总体为各个分布的条件概率；与 k-means 不同的是，在 M 步即估算参数的过程中，不仅仅采用当前认为属于该分布的样本，而是使用所有样本；只是样本对参数的影响，取决于此前计算的条件概率。

相比 k-means，EM 的优点是有全局最优解，且不局限于**球形**总体。缺点是由于每个迭代需要做参数估计，计算速度较慢。

### 高斯混合模型

当多元分布选取为高斯分布时，EM 算法又称为高斯混合模型。

<!-- more -->

输入：$n$ 维向量 $x$ 的观测数据 $x_1,x_2,…,x_m$，分布的个数 $K$

输出：$K$ 个分布的参数

(1) 初始化：将观测数据随机分配到 $K$ 组。对每一组，仅使用组内的样本做高斯分布的参数估计，得到各组的初始化参数。

(2) E 步：根据当前参数，计算任一样本属于每个分布的先验概率

$$\theta_k = \frac{1}{m} \sum_{j=1}^m w_{jk}, \ k=1,2,…,K$$

其中在初始化后，$w_{jk} \in \{0, 1\}$，即在初始分配的组内值为1，在其他组值为0。

以 $\Phi$ 表示高斯分布的密度函数，计算在每个样本条件下，分布属于各个分布的条件概率

> $n$ 维空间下高斯分布的密度函数 $$\Phi(x) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} \exp(\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu))$$

$$w_{jk} = P(\mu_k, \Sigma_k | x_j) = \frac{\theta_k \Phi(x_j | \mu_k, \Sigma_k)}{\sum_{k=1}^K \theta_k \Phi(x_j | \mu_k, \Sigma_k)}$$

(3) M 步：以 $w_{jk}$ 为样本权重，更新每个高斯分布的参数，即均值向量和协方差矩阵

$$\mu_k := \frac{\sum_{j=1}^m w_{jk} x_j}{\sum_{j=1}^m w_{jk}}$$

$$\Sigma_k := \frac{\sum_{j=1}^m w_{jk} (x_j - \mu_k)(x_j - \mu_k)^T}{\sum_{j=1}^m w_{jk}}$$

## Python 实现

实现代码参见 [GitHub](https://github.com/ferris-wufei/algorithm_ml/blob/master/ml_EM.py)